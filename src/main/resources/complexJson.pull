#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#########################################################################
###################### Demo job configuration file ######################
#########################################################################

job.name=complexJson
job.group=NestedJson
job.description=A Gobblin job for complex json ingestion

source.class=com.gobblin.core.ComplexJsonSource
converter.classes=com.gobblin.core.ComplexJsonConverter
extract.namespace=ComplexJsonExtractor

# source configuration properties
# comma-separated list of file URIs (supporting different schemes, e.g., file://, ftp://, sftp://, http://, etc)
source.filebased.files.to.pull=file:///opt/gobblin/distributions/resources/complexJson.json

# whether to use authentication or not (default is false)
source.conn.use.authentication=
# credential for authentication purpose (optional)
source.conn.domain=
source.conn.username=
source.conn.password=

# source schema
converter.avroSchema={"namespace" : "evoc.avro","type" : "record","name" : "evoc","fields" : [{"name" : "project","type" : "string","isNullable" : "false"},{"name" : "description","type" : "string","isNullable" : "false"},{"name" : "start_date","type" : "string","isNullable" : "true"},{"name" : "end_date","type" : "string","isNullable" : "true"},{"name" : "employees","type" : {"type" : "array","items" : {"name" : "employee","type" : "record","fields" : [{"name" : "name","type" : "string","isNullable" : "true"},{"name" : "designation","type" : "string","isNullable" : "true"},{"name" : "start_date","type" : "string"},{"name" : "end_date","type" : "string"}]}}}]}

# converter avro schema key
avro.schema.literal={"namespace" : "evoc.avro","type" : "record","name" : "evoc","fields" : [{"name" : "project","type" : "string","isNullable" : "false"},{"name" : "description","type" : "string","isNullable" : "false"},{"name" : "start_date","type" : "string","isNullable" : "true"},{"name" : "end_date","type" : "string","isNullable" : "true"},{"name" : "employees","type" : {"type" : "array","items" : {"name" : "employee","type" : "record","fields" : [{"name" : "name","type" : "string","isNullable" : "true"},{"name" : "designation","type" : "string","isNullable" : "true"},{"name" : "start_date","type" : "string"},{"name" : "end_date","type" : "string"}]}}}]}
converter.ignoreFields=

# quality checker configuration properties
qualitychecker.task.policies=org.apache.gobblin.policies.count.RowCountPolicy,org.apache.gobblin.policies.schema.SchemaCompatibilityPolicy
qualitychecker.task.policy.types=OPTIONAL,OPTIONAL
qualitychecker.row.policies=org.apache.gobblin.policies.schema.SchemaRowCheckPolicy
qualitychecker.row.policy.types=OPTIONAL
qualitychecker.row.err.file=test/jobOutput

# data publisher class to be used
data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher

# Hive serializer properties
serde.deserializer.type=AVRO
serde.serializer.type=ORC

# Hive Properties
hive.dataset.database=assignment
hive.dataset.hive.metastore.uri=thrift://sandbox.hortonworks.com:9083
hiveserver.connection.string=jdbc:hive2://sandbox.hortonworks.com:10000

# writer configuration properties
writer.destination.type=HDFS
writer.output.format=AVRO
writer.fs.uri=hdfs://sandbox.hortonworks.com:8020
writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder

mr.job.root.dir=hdfs://sandbox.hortonworks.com:8020/gobblin/complexJsonDir/working
state.store.dir=file:///root/gobblin/complexJsonDir/state-store
task.data.root.dir=hdfs://sandbox.hortonworks.com:8020/gobblin/complexJsonDir/task-data
writer.staging.dir=hdfs://sandbox.hortonworks.com:8020/gobblin/complexJsonDir/task-staging
writer.output.dir=hdfs://sandbox.hortonworks.com:8020/gobblin/complexJsonDir/task-output
data.publisher.final.dir=hdfs://sandbox.hortonworks.com:8020/gobblin/complexJsonDir

# Task properties
task.maxretries=2

# Fork Properties
fork.record.queue.capacity=1